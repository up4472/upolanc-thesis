{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f08f55-17e9-4f66-b952-60801d8cc696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import numpy\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84acd8e-25d9-4238-b0e5-9273e7ec66a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure source path\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "\n",
    "while not ROOT.endswith('upolanc-thesis') :\n",
    "\tROOT = os.path.abspath(os.path.join(ROOT, os.pardir))\n",
    "\n",
    "\tif len(ROOT) < len('upolanc-thesis') :\n",
    "\t\tif   platform.system() == 'Linux' :\n",
    "\t\t\tROOT = '/d/hpc/home/up4472/workspace/upolanc-thesis'\n",
    "\t\telif platform.system() == 'Windows' :\n",
    "\t\t\tROOT = 'C:\\\\Developer\\\\Workspace\\\\PyCharm\\\\Projects\\\\upolanc-thesis'\n",
    "\t\telse :\n",
    "\t\t\traise ValueError()\n",
    "\n",
    "\t\tprint(f'Warning : could not find correct directory, using default : {ROOT}')\n",
    "\t\tbreak\n",
    "\n",
    "if ROOT not in sys.path :\n",
    "\tsys.path.append(ROOT)\n",
    "\n",
    "os.chdir(ROOT)\n",
    "\n",
    "sys.path.append(os.path.join(ROOT, 'source', 'python', 'bert', 'dnabert', 'src'))\n",
    "sys.path.append(os.path.join(ROOT, 'source', 'python', 'bert', 'dnabert', 'src', 'transformers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed655e0d-e4c1-429e-9b1b-e969359bba76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformers\n",
    "\n",
    "from transformers.data.processors.glue import DnaPromProcessor\n",
    "from transformers.data.processors.glue import StsbProcessor\n",
    "\n",
    "from transformers import WEIGHTS_NAME\n",
    "from transformers import AdamW\n",
    "from transformers import AlbertConfig\n",
    "from transformers import AlbertForSequenceClassification\n",
    "from transformers import AlbertTokenizer\n",
    "from transformers import BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertForLongSequenceClassification\n",
    "from transformers import BertForLongSequenceClassificationCat\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DNATokenizer\n",
    "from transformers import DistilBertConfig\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import FlaubertConfig\n",
    "from transformers import FlaubertForSequenceClassification\n",
    "from transformers import FlaubertTokenizer\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import XLMConfig\n",
    "from transformers import XLMForSequenceClassification\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from transformers import XLMTokenizer\n",
    "from transformers import XLNetConfig\n",
    "from transformers import XLNetForSequenceClassification\n",
    "from transformers import XLNetTokenizer\n",
    "from transformers import DataProcessor\n",
    "from transformers import InputExample\n",
    "from transformers import BertModel\n",
    "from transformers import BertPreTrainedModel\n",
    "\n",
    "from transformers import glue_convert_examples_to_features  as convert_examples_to_features\n",
    "from transformers import glue_output_modes                  as output_modes\n",
    "from transformers import glue_processors                    as processors\n",
    "from transformers import glue_compute_metrics               as compute_metrics\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "try :\n",
    "\tfrom torch.utils.tensorboard import SummaryWriter\n",
    "except :\n",
    "\tfrom tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1581a0-397e-4c6c-97a9-28b0b606de0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Other\n",
    "\n",
    "from multiprocessing              import Pool\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data             import DataLoader\n",
    "from torch.utils.data             import RandomSampler\n",
    "from torch.utils.data             import SequentialSampler\n",
    "from torch.utils.data             import TensorDataset\n",
    "from torch.nn                     import Softmax\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cce6a-809d-48fd-9664-6761154ee85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "\n",
    "from source.python.bert.bert_models     import RegressionBertFC1\n",
    "from source.python.bert.bert_models     import RegressionBertFC3\n",
    "from source.python.bert.bert_processors import RegressionProcessor\n",
    "from source.python.bert.bert_metrics    import compute_metrics\n",
    "from source.python                      import runtime\n",
    "\n",
    "runtime.set_numpy_format()\n",
    "runtime.set_pandas_format()\n",
    "runtime.set_plot_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430bb6b-342c-49ed-ba1e-c8801389487b",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3911002-f455-4bb3-aeca-3a314a1ba9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logger\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703ef10-9aa1-4c71-a293-d0f91582207c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update models\n",
    "\n",
    "ALL_MODELS = sum((\n",
    "\ttuple(conf.pretrained_config_archive_map.keys())\n",
    "\tfor conf in (\n",
    "\t\t      BertConfig,  XLNetConfig,        XLMConfig,  RobertaConfig,\n",
    "\t\tDistilBertConfig, AlbertConfig, XLMRobertaConfig, FlaubertConfig\n",
    "\t)),\n",
    "\t()\n",
    ")\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "\t\"dna\"        : (      BertConfig,       BertForSequenceClassification,        DNATokenizer),\n",
    "\t\"dnalong\"    : (      BertConfig,   BertForLongSequenceClassification,        DNATokenizer),\n",
    "\t\"dnalongcat\" : (      BertConfig,   BertForLongSequenceClassification,        DNATokenizer),\n",
    "\t\"bert\"       : (      BertConfig,       BertForSequenceClassification,       BertTokenizer),\n",
    "\t\"xlnet\"      : (     XLNetConfig,      XLNetForSequenceClassification,      XLNetTokenizer),\n",
    "\t\"xlm\"        : (       XLMConfig,        XLMForSequenceClassification,        XLMTokenizer),\n",
    "\t\"roberta\"    : (   RobertaConfig,    RobertaForSequenceClassification,    RobertaTokenizer),\n",
    "\t\"distilbert\" : (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),\n",
    "\t\"albert\"     : (    AlbertConfig,     AlbertForSequenceClassification,     AlbertTokenizer),\n",
    "\t\"xlmroberta\" : (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer),\n",
    "\t\"flaubert\"   : (  FlaubertConfig,   FlaubertForSequenceClassification,   FlaubertTokenizer),\n",
    "\t'rbertfc1'   : (      BertConfig,                   RegressionBertFC1,        DNATokenizer),\n",
    "\t'rbertfc3'   : (      BertConfig,                   RegressionBertFC3,        DNATokenizer)\n",
    "}\n",
    "\n",
    "TOKEN_ID_GROUP = [\n",
    "\t'bert',\n",
    "\t'dnalong',\n",
    "\t'dnalongcat',\n",
    "\t'xlnet',\n",
    "\t'albert'\n",
    "]\n",
    "\n",
    "print('Added model_type = <rbertfc1>   :: bert for sequence regression with 1 FC layer')\n",
    "print('Added model_type = <rbertfc3>   :: bert for sequence regression with 3 FC layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dcf5b-d032-47b5-a2ee-6bfd4f62acbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update processors and output modes\n",
    "\n",
    "processors   = processors   | {'regression' : RegressionProcessor}\n",
    "output_modes = output_modes | {'regression' : 'regression'}\n",
    "\n",
    "print('Added task_type  = <regression> :: defines regression output_mode and processor')\n",
    "print()\n",
    "\n",
    "for k, v in processors.items() :\n",
    "\tprint('{:12s} : {}'.format(k, v))\n",
    "\n",
    "print()\n",
    "\n",
    "for k, v in output_modes.items() :\n",
    "\tprint('{:12s} : {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5b685-1658-4c5c-ab91-ad0e79c1f665",
   "metadata": {},
   "source": [
    "# 2. Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f525d2-c6d2-4908-893b-d0f31a023126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed (args) :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\trandom.seed(args.seed)\n",
    "\tnumpy.random.seed(args.seed)\n",
    "\ttorch.manual_seed(args.seed)\n",
    "\n",
    "\tif args.n_gpu > 0 :\n",
    "\t\ttorch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fc0bc-2b25-4814-8dbf-544b40345921",
   "metadata": {},
   "source": [
    "# 3. Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2c422-c58e-4abd-9858-8d7ee2e0196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sorted_checkpoints (args, checkpoint_prefix = 'checkpoint', use_mtime = False) :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\tordering_and_checkpoint_path = list()\n",
    "\tglob_checkpoints = glob.glob(os.path.join(args.output_dir, '{}-*'.format(checkpoint_prefix)))\n",
    "\n",
    "\tfor path in glob_checkpoints :\n",
    "\t\tif use_mtime :\n",
    "\t\t\tordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "\t\telse :\n",
    "\t\t\tregex_match = re.match('.*{}-([0-9]+)'.format(checkpoint_prefix), path)\n",
    "\n",
    "\t\t\tif regex_match and regex_match.groups() :\n",
    "\t\t\t\tordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "\tcheckpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "\tcheckpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "\n",
    "\treturn checkpoints_sorted\n",
    "\n",
    "def _rotate_checkpoints (args, checkpoint_prefix = 'checkpoint', use_mtime = False) :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\tif not args.save_total_limit :\n",
    "\t\treturn\n",
    "\tif args.save_total_limit <= 0 :\n",
    "\t\treturn\n",
    "\n",
    "\t# Check if we should delete older checkpoint(s)\n",
    "\tcheckpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "\n",
    "\tif len(checkpoints_sorted) <= args.save_total_limit :\n",
    "\t\treturn\n",
    "\n",
    "\tnumber_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "\tcheckpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "\n",
    "\tfor checkpoint in checkpoints_to_be_deleted :\n",
    "\t\tlogger.info('Deleting older checkpoint [{}] due to args.save_total_limit'.format(checkpoint))\n",
    "\t\tshutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c296e0a-8028-4db9-a2a9-6100ddda5cc7",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6490a2c-eba4-44d0-adf2-366dd1883a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (args, train_dataset, model, tokenizer) :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\tif args.local_rank in [-1, 0] :\n",
    "\t\ttb_writer = SummaryWriter()\n",
    "\n",
    "\targs.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "\ttrain_sampler    = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "\ttrain_dataloader = DataLoader(train_dataset, sampler = train_sampler, batch_size = args.train_batch_size)\n",
    "\n",
    "\tif args.max_steps > 0 :\n",
    "\t\tt_total = args.max_steps\n",
    "\t\targs.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "\telse :\n",
    "\t\tt_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "\t#\n",
    "\t# Prepare optimizer and schedule (linear warmup and decay)\n",
    "\t#\n",
    "\n",
    "\tno_decay = ['bias', 'LayerNorm.weight']\n",
    "\toptimizer_grouped_parameters = [\n",
    "\t\t{\n",
    "\t\t\t'params'       : [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "\t\t\t'weight_decay' : args.weight_decay,\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'params'       : [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "\t\t\t'weight_decay' : 0.0\n",
    "\t\t},\n",
    "\t]\n",
    "\n",
    "\twarmup_steps = args.warmup_steps if args.warmup_percent == 0 else int(args.warmup_percent * t_total)\n",
    "\n",
    "\toptimizer = AdamW(optimizer_grouped_parameters, lr = args.learning_rate, eps = args.adam_epsilon, betas = (args.beta1, args.beta2))\n",
    "\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps = t_total)\n",
    "\n",
    "\t#\n",
    "\t# Check if saved optimizer or scheduler states exist\n",
    "\t#\n",
    "\n",
    "\tif os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')) :\n",
    "\t\toptimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n",
    "\t\tscheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n",
    "\n",
    "\tif args.fp16 :\n",
    "\t\ttry :\n",
    "\t\t\tfrom apex import amp\n",
    "\t\texcept ImportError :\n",
    "\t\t\traise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n",
    "\n",
    "\t\tmodel, optimizer = amp.initialize(model, optimizer, opt_level = args.fp16_opt_level)\n",
    "\n",
    "\t#\n",
    "\t# Multi GPU traninig\n",
    "\t#\n",
    "\n",
    "\tif args.n_gpu > 1 :\n",
    "\t\tmodel = torch.nn.DataParallel(model)\n",
    "\n",
    "\t#\n",
    "\t# Distributed training (should be after apex fp16 initialization)\n",
    "\t#\n",
    "\n",
    "\tif args.local_rank != -1 :\n",
    "\t\tmodel = torch.nn.parallel.DistributedDataParallel(model,\n",
    "\t\t\tdevice_ids    = [args.local_rank],\n",
    "\t\t\toutput_device = args.local_rank,\n",
    "\t\t\tfind_unused_parameters = True,\n",
    "\t\t)\n",
    "\n",
    "\t#\n",
    "\t# Start training\n",
    "\t#\n",
    "\n",
    "\tlogger.info('***** Running training *****')\n",
    "\tlogger.info('  Num examples = %d', len(train_dataset))\n",
    "\tlogger.info('  Num Epochs = %d', args.num_train_epochs)\n",
    "\tlogger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n",
    "\n",
    "\tlogger.info(\n",
    "\t\t'  Total train batch size (w. parallel, distributed & accumulation) = %d',\n",
    "\t\targs.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1)\n",
    "\t)\n",
    "\n",
    "\tlogger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n",
    "\tlogger.info('  Total optimization steps = %d', t_total)\n",
    "\n",
    "\tglobal_step = 0\n",
    "\tepochs_trained = 0\n",
    "\tsteps_trained_in_current_epoch = 0\n",
    "\n",
    "\t#\n",
    "\t# Check for checkpoints\n",
    "\t#\n",
    "\n",
    "\tif os.path.exists(args.model_name_or_path) :\n",
    "\t\ttry :\n",
    "\t\t\tglobal_step = int(args.model_name_or_path.split('-')[-1].split('/')[0])\n",
    "\t\texcept :\n",
    "\t\t\tglobal_step = 0\n",
    "\n",
    "\t\tepochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\t\tsteps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "\t\tlogger.info('  Continuing training from checkpoint, will skip to saved global_step')\n",
    "\t\tlogger.info('  Continuing training from epoch %d', epochs_trained)\n",
    "\t\tlogger.info('  Continuing training from global step %d', global_step)\n",
    "\t\tlogger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n",
    "\n",
    "\ttr_loss      = 0.0\n",
    "\tlogging_loss = 0.0\n",
    "\n",
    "\tmodel.zero_grad()\n",
    "\n",
    "\ttrain_iterator = trange(epochs_trained, int(args.num_train_epochs), desc = 'Epoch', disable = args.local_rank not in [-1, 0])\n",
    "\tset_seed(args)\n",
    "\n",
    "\tbest_auc = 0\n",
    "\tlast_auc = 0\n",
    "\tstop_count = 0\n",
    "\n",
    "\t#\n",
    "\t# Train loop\n",
    "\t#\n",
    "\n",
    "\tfor _ in train_iterator :\n",
    "\t\tepoch_iterator = tqdm(train_dataloader, desc = 'Iteration', disable = args.local_rank not in [-1, 0])\n",
    "\n",
    "\t\tfor step, batch in enumerate(epoch_iterator) :\n",
    "\t\t\tif steps_trained_in_current_epoch > 0 :\n",
    "\t\t\t\tsteps_trained_in_current_epoch -= 1\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tmodel.train()\n",
    "\n",
    "\t\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
    "\t\t\tinputs = {\n",
    "\t\t\t\t'input_ids'      : batch[0],\n",
    "\t\t\t\t'attention_mask' : batch[1],\n",
    "\t\t\t\t'labels'         : batch[3]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\t# XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "\t\t\tif args.model_type != 'distilbert' :\n",
    "\t\t\t\tinputs['token_type_ids'] = (batch[2] if args.model_type in TOKEN_ID_GROUP else None)\n",
    "\n",
    "\t\t\toutputs = model(**inputs)\n",
    "\t\t\tloss = outputs[0] \n",
    "\n",
    "\t\t\tif args.n_gpu > 1 :\n",
    "\t\t\t\tloss = loss.mean()\n",
    "\t\t\tif args.gradient_accumulation_steps > 1 :\n",
    "\t\t\t\tloss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "\t\t\tif args.fp16 :\n",
    "\t\t\t\twith amp.scale_loss(loss, optimizer) as scaled_loss :\n",
    "\t\t\t\t\tscaled_loss.backward()\n",
    "\t\t\telse :\n",
    "\t\t\t\tloss.backward()\n",
    "\n",
    "\t\t\ttr_loss += loss.item()\n",
    "\n",
    "\t\t\tif (step + 1) % args.gradient_accumulation_steps == 0 :\n",
    "\t\t\t\tif args.fp16 :\n",
    "\t\t\t\t\ttorch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tscheduler.step()\n",
    "\t\t\t\tmodel.zero_grad()\n",
    "\n",
    "\t\t\t\tglobal_step += 1\n",
    "\n",
    "\t\t\t\tif args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0 :\n",
    "\t\t\t\t\tlogs = {}\n",
    "\n",
    "\t\t\t\t\t# Only evaluate when single GPU otherwise metrics may not average well\n",
    "\t\t\t\t\tif (args.local_rank == -1 and args.evaluate_during_training) :\n",
    "\t\t\t\t\t\tresults = evaluate(args, model, tokenizer)\n",
    "\n",
    "\t\t\t\t\t\tif args.task_name == 'dna690' :\n",
    "\t\t\t\t\t\t\tif results['auc'] > best_auc:\n",
    "\t\t\t\t\t\t\t\tbest_auc = results['auc']\n",
    "\n",
    "\t\t\t\t\t\t#\n",
    "\t\t\t\t\t\t# Early stopping with AUC ?\n",
    "\t\t\t\t\t\t#\n",
    "\n",
    "\t\t\t\t\t\tif args.early_stop != 0 :\n",
    "\t\t\t\t\t\t\tif results['auc'] < last_auc :\n",
    "\t\t\t\t\t\t\t\tstop_count = stop_count + 1\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tstop_count = 0\n",
    "\n",
    "\t\t\t\t\t\t\tlast_auc = results['auc']\n",
    "\n",
    "\t\t\t\t\t\t\tif stop_count == args.early_stop :\n",
    "\t\t\t\t\t\t\t\tlogger.info('Early stop')\n",
    "\n",
    "\t\t\t\t\t\t\t\treturn global_step, tr_loss / global_step\n",
    "\n",
    "\t\t\t\t\t\tfor key, value in results.items():\n",
    "\t\t\t\t\t\t\teval_key = 'eval_{}'.format(key)\n",
    "\t\t\t\t\t\t\tlogs[eval_key] = value\n",
    "\n",
    "\t\t\t\t\tloss_scalar = (tr_loss - logging_loss) / args.logging_steps\n",
    "\t\t\t\t\tlearning_rate_scalar = scheduler.get_lr()[0]\n",
    "\n",
    "\t\t\t\t\tlogs['learning_rate'] = learning_rate_scalar\n",
    "\t\t\t\t\tlogs['loss'] = loss_scalar\n",
    "\n",
    "\t\t\t\t\tlogging_loss = tr_loss\n",
    "\n",
    "\t\t\t\t\tfor key, value in logs.items() :\n",
    "\t\t\t\t\t\ttb_writer.add_scalar(key, value, global_step)\n",
    "\n",
    "\t\t\t\t\tprint({**logs, **{'step' : global_step}})\n",
    "\n",
    "\t\t\t\tif args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0 :\n",
    "\t\t\t\t\tif args.task_name == 'dna690' and results['auc'] < best_auc :\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t# Save model checkpoint\n",
    "\t\t\t\t\tcheckpoint_prefix = 'checkpoint'\n",
    "\t\t\t\t\toutput_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n",
    "\n",
    "\t\t\t\t\tif not os.path.exists(output_dir) :\n",
    "\t\t\t\t\t\tos.makedirs(output_dir)\n",
    "\n",
    "\t\t\t\t\tmodel_to_save = (model.module if hasattr(model, 'module') else model)\n",
    "\t\t\t\t\tmodel_to_save.save_pretrained(output_dir)\n",
    "\t\t\t\t\ttokenizer.save_pretrained(output_dir)\n",
    "\n",
    "\t\t\t\t\tlogger.info('Saving model checkpoint to %s', output_dir)\n",
    "\n",
    "\t\t\t\t\t_rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "\t\t\t\t\tif args.task_name != 'dna690' :\n",
    "\t\t\t\t\t\ttorch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "\t\t\t\t\t\ttorch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
    "\t\t\t\t\t\ttorch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n",
    "\n",
    "\t\t\t\t\tlogger.info('Saving optimizer and scheduler states to %s', output_dir)\n",
    "\n",
    "\t\t\tif args.max_steps > 0 and global_step > args.max_steps :\n",
    "\t\t\t\tepoch_iterator.close()\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tif args.max_steps > 0 and global_step > args.max_steps :\n",
    "\t\t\ttrain_iterator.close()\n",
    "\t\t\tbreak\n",
    "\n",
    "\tif args.local_rank in [-1, 0] :\n",
    "\t\ttb_writer.close()\n",
    "\n",
    "\treturn global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00994df7-0b48-4c83-a77d-8dabc90ff4bb",
   "metadata": {},
   "source": [
    "# 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee898c-1361-4872-aa34-7243cbf9215c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate (args, model, tokenizer, prefix = '', evaluate = True) :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "\teval_task_names = ('mnli', 'mnli-mm') if args.task_name == 'mnli' else (args.task_name,)\n",
    "\teval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == 'mnli' else (args.output_dir,)\n",
    "\n",
    "\tif args.task_name[:3] == 'dna' :\n",
    "\t\tsoftmax = Softmax(dim = 1)\n",
    "\n",
    "\tresults = {}\n",
    "\n",
    "\tfor eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs) :\n",
    "\t\teval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=evaluate)\n",
    "\n",
    "\t\tif not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0] :\n",
    "\t\t\tos.makedirs(eval_output_dir)\n",
    "\n",
    "\t\targs.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "\t\teval_sampler    = SequentialSampler(eval_dataset)\n",
    "\t\teval_dataloader = DataLoader(eval_dataset, sampler = eval_sampler, batch_size = args.eval_batch_size)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Multi GPU evaluation\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel) :\n",
    "\t\t\tmodel = torch.nn.DataParallel(model)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Start evaluation\n",
    "\t\t#\n",
    "\n",
    "\t\tlogger.info('***** Running evaluation {} *****'.format(prefix))\n",
    "\t\tlogger.info('  Num examples = %d', len(eval_dataset))\n",
    "\t\tlogger.info('  Batch size = %d', args.eval_batch_size)\n",
    "\n",
    "\t\teval_loss     = 0.0\n",
    "\t\tnb_eval_steps = 0\n",
    "\t\tpreds         = None\n",
    "\t\tprobs         = None\n",
    "\t\tout_label_ids = None\n",
    "\n",
    "\t\t#\n",
    "\t\t# Evaluation loop\n",
    "\t\t#\n",
    "\n",
    "\t\tfor batch in tqdm(eval_dataloader, desc = 'Evaluating') :\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "\t\t\twith torch.no_grad() :\n",
    "\t\t\t\tinputs = {\n",
    "\t\t\t\t\t'input_ids'      : batch[0],\n",
    "\t\t\t\t\t'attention_mask' : batch[1],\n",
    "\t\t\t\t\t'labels'         : batch[3]\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\t\t\t# XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "\t\t\t\tif args.model_type != 'distilbert' :\n",
    "\t\t\t\t\tinputs['token_type_ids'] = (batch[2] if args.model_type in TOKEN_ID_GROUP else None)\n",
    "\n",
    "\t\t\t\toutputs = model(**inputs)\n",
    "\t\t\t\ttmp_eval_loss, logits = outputs[:2]\n",
    "\t\t\t\teval_loss += tmp_eval_loss.mean().item()\n",
    "\t\n",
    "\t\t\tnb_eval_steps += 1\n",
    "\n",
    "\t\t\tif preds is None :\n",
    "\t\t\t\tpreds = logits.detach().cpu().numpy()\n",
    "\t\t\t\tout_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "\t\t\telse :\n",
    "\t\t\t\tpreds = numpy.append(preds, logits.detach().cpu().numpy(), axis = 0)\n",
    "\t\t\t\tout_label_ids = numpy.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis = 0)\n",
    "\n",
    "\t\teval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "\t\t#\n",
    "\t\t# Handle classification tasks\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.output_mode == 'classification' :\n",
    "\t\t\tif args.task_name[:3] == 'dna' and args.task_name != 'dnasplice' :\n",
    "\t\t\t\tif args.do_ensemble_pred :\n",
    "\t\t\t\t\tprobs = softmax(torch.tensor(preds, dtype=torch.float32)).numpy()\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tprobs = softmax(torch.tensor(preds, dtype=torch.float32))[:, 1].numpy()\n",
    "\t\t\telif args.task_name == 'dnasplice' :\n",
    "\t\t\t\tprobs = softmax(torch.tensor(preds, dtype=torch.float32)).numpy()\n",
    "\n",
    "\t\t\tpreds = numpy.argmax(preds, axis = 1)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Handle regression tasks\n",
    "\t\t#\n",
    "\n",
    "\t\telif args.output_mode == 'regression' :\n",
    "\t\t\tpreds = numpy.squeeze(preds)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Compute metrics\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.do_ensemble_pred :\n",
    "\t\t\tresult = compute_metrics(eval_task, preds, out_label_ids, probs[:, 1])\n",
    "\t\telse :\n",
    "\t\t\tresult = compute_metrics(eval_task, preds, out_label_ids, probs)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Update results\n",
    "\t\t#\n",
    "\n",
    "\t\tresults.update(result)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Handle DNA690 task\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.task_name == 'dna690' :\n",
    "\t\t\teval_output_dir = args.result_dir\n",
    "\n",
    "\t\t\tif not os.path.exists(args.result_dir) :\n",
    "\t\t\t\tos.makedirs(args.result_dir)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Log evaluation results\n",
    "\t\t#\n",
    "\n",
    "\t\toutput_eval_file = os.path.join(eval_output_dir, prefix, 'eval_results.txt')\n",
    "\n",
    "\t\twith open(output_eval_file, 'a') as writer :\n",
    "\t\t\tif args.task_name[:3] == 'dna' :\n",
    "\t\t\t\teval_result = args.data_dir.split('/')[-1] + ' '\n",
    "\t\t\telse:\n",
    "\t\t\t\teval_result = ''\n",
    "\n",
    "\t\t\tlogger.info('***** Eval results {} *****'.format(prefix))\n",
    "\n",
    "\t\t\tfor key in sorted(result.keys()) :\n",
    "\t\t\t\tlogger.info('  %s = %s', key, str(result[key]))\n",
    "\t\t\t\teval_result = eval_result + str(result[key])[:5] + ' '\n",
    "\n",
    "\t\t\twriter.write(eval_result + '\\n')\n",
    "\n",
    "\tif args.do_ensemble_pred :\n",
    "\t\treturn results, eval_task, preds, out_label_ids, probs\n",
    "\telse:\n",
    "\t\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26cb594-7b28-474f-ad4c-2c574e6cb41c",
   "metadata": {},
   "source": [
    "# 6. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149940dd-596d-4349-a3c7-8504f1ae413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (args, model, tokenizer, prefix = '') :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "\tpred_task_names   = (args.task_name,)\n",
    "\tpred_outputs_dirs = (args.predict_dir,)\n",
    "\n",
    "\tif not os.path.exists(args.predict_dir) :\n",
    "\t\tos.makedirs(args.predict_dir)\n",
    "\n",
    "\tsoftmax = Softmax(dim = 1)\n",
    "\n",
    "\tpredictions = {}\n",
    "\n",
    "\tfor pred_task, pred_output_dir in zip(pred_task_names, pred_outputs_dirs) :\n",
    "\t\tpred_dataset = load_and_cache_examples(args, pred_task, tokenizer, evaluate=True)\n",
    "\n",
    "\t\tif not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0] :\n",
    "\t\t\tos.makedirs(pred_output_dir)\n",
    "\n",
    "\t\targs.pred_batch_size = args.per_gpu_pred_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "\t\tpred_sampler    = SequentialSampler(pred_dataset)\n",
    "\t\tpred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=args.pred_batch_size)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Multi GPU prediction\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel) :\n",
    "\t\t\tmodel = torch.nn.DataParallel(model)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Stat prediction\n",
    "\t\t#\n",
    "\n",
    "\t\tlogger.info('***** Running prediction {} *****'.format(prefix))\n",
    "\t\tlogger.info('  Num examples = %d', len(pred_dataset))\n",
    "\t\tlogger.info('  Batch size = %d', args.pred_batch_size)\n",
    "\n",
    "\t\tpred_loss     = 0.0\n",
    "\t\tnb_pred_steps = 0\n",
    "\t\tpreds         = None\n",
    "\t\tout_label_ids = None\n",
    "\n",
    "\t\tfor batch in tqdm(pred_dataloader, desc = 'Predicting') :\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "\t\t\twith torch.no_grad() :\n",
    "\t\t\t\tinputs = {\n",
    "\t\t\t\t\t'input_ids'      : batch[0],\n",
    "\t\t\t\t\t'attention_mask' : batch[1],\n",
    "\t\t\t\t\t'labels'         : batch[3]\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\t\t\t# XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "\t\t\t\tif args.model_type != 'distilbert' :\n",
    "\t\t\t\t\tinputs['token_type_ids'] = (batch[2] if args.model_type in TOKEN_ID_GROUP else None)\n",
    "\n",
    "\t\t\t\toutputs = model(**inputs)\n",
    "\t\t\t\t_, logits = outputs[:2]\n",
    "\n",
    "\t\t\tif preds is None :\n",
    "\t\t\t\tpreds = logits.detach().cpu().numpy()\n",
    "\t\t\t\tout_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "\t\t\telse :\n",
    "\t\t\t\tpreds = numpy.append(preds, logits.detach().cpu().numpy(), axis = 0)\n",
    "\t\t\t\tout_label_ids = numpy.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis = 0)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Handle classification tasks\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.output_mode == 'classification' :\n",
    "\t\t\tif args.task_name[:3] == 'dna' and args.task_name != 'dnasplice' :\n",
    "\t\t\t\tif args.do_ensemble_pred :\n",
    "\t\t\t\t\tprobs = softmax(torch.tensor(preds, dtype = torch.float32)).numpy()\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tprobs = softmax(torch.tensor(preds, dtype = torch.float32))[:, 1].numpy()\n",
    "\t\t\telif args.task_name == 'dnasplice' :\n",
    "\t\t\t\tprobs = softmax(torch.tensor(preds, dtype = torch.float32)).numpy()\n",
    "\n",
    "\t\t\tpreds = numpy.argmax(preds, axis = 1)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Handle regression tasks\n",
    "\t\t#\n",
    "\n",
    "\t\telif args.output_mode == 'regression' :\n",
    "\t\t\tpreds = numpy.squeeze(preds)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Compute metrics\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.do_ensemble_pred :\n",
    "\t\t\tresult = compute_metrics(pred_task, preds, out_label_ids, probs[:, 1])\n",
    "\t\telse :\n",
    "\t\t\tresult = compute_metrics(pred_task, preds, out_label_ids, probs)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Log prediction results\n",
    "\t\t#\n",
    "\n",
    "\t\tpred_output_dir = args.predict_dir\n",
    "\n",
    "\t\tif not os.path.exists(pred_output_dir) :\n",
    "\t\t\tos.makedir(pred_output_dir)\n",
    "\n",
    "\t\toutput_pred_file = os.path.join(pred_output_dir, 'pred_results.npy')\n",
    "\t\tlogger.info('***** Pred results {} *****'.format(prefix))\n",
    "\n",
    "\t\tfor key in sorted(result.keys()) :\n",
    "\t\t\tlogger.info('  %s = %s', key, str(result[key]))\n",
    "\n",
    "\t\tnumpy.save(output_pred_file, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76de7e-bd4d-45ad-9ad5-cc178df66029",
   "metadata": {},
   "source": [
    "# 7. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4aa31-c084-4d77-9050-6a77c39b7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_attention (attention) :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\tsqueezed = list()\n",
    "\n",
    "\tfor layer_attention in attention :\n",
    "\t\t# 1 x num_heads x seq_len x seq_len\n",
    "\t\tif len(layer_attention.shape) != 4 :\n",
    "\t\t\traise ValueError('The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model.')\n",
    "\n",
    "\t\tsqueezed.append(layer_attention.squeeze(0))\n",
    "\n",
    "\t# num_layers x num_heads x seq_len x seq_len\n",
    "\treturn torch.stack(squeezed).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6101f5-a794-4790-88be-960cf5cac211",
   "metadata": {},
   "source": [
    "# 8. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1a225-0b8c-4b65-b213-a621deda91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize (args, model, tokenizer, kmer, prefix = '') :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\tpred_task_names   = (args.task_name,)\n",
    "\tpred_outputs_dirs = (args.predict_dir,)\n",
    "\n",
    "\tif not os.path.exists(args.predict_dir) :\n",
    "\t\tos.makedirs(args.predict_dir)\n",
    "\n",
    "\tsoftmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "\tfor pred_task, pred_output_dir in zip(pred_task_names, pred_outputs_dirs) :\n",
    "\t\tevaluate     = False if args.visualize_train else True\n",
    "\t\tpred_dataset = load_and_cache_examples(args, pred_task, tokenizer, evaluate = evaluate)\n",
    "\n",
    "\t\tif not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0] :\n",
    "\t\t\tos.makedirs(pred_output_dir)\n",
    "\n",
    "\t\targs.pred_batch_size = args.per_gpu_pred_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "\t\tpred_sampler    = SequentialSampler(pred_dataset)\n",
    "\t\tpred_dataloader = DataLoader(pred_dataset, sampler = pred_sampler, batch_size = args.pred_batch_size)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Multi GPU visualization\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel) :\n",
    "\t\t\tmodel = torch.nn.DataParallel(model)\n",
    "\n",
    "\t\t#\n",
    "\t\t# Start prediciton\n",
    "\t\t#\n",
    "\n",
    "\t\tlogger.info('***** Running prediction {} *****'.format(prefix))\n",
    "\t\tlogger.info('  Num examples = %d', len(pred_dataset))\n",
    "\t\tlogger.info('  Batch size = %d', args.pred_batch_size)\n",
    "\n",
    "\t\tpred_loss     = 0.0\n",
    "\t\tnb_pred_steps = 0\n",
    "\t\tbatch_size    = args.pred_batch_size\n",
    "\n",
    "\t\tif args.task_name != 'dnasplice' :\n",
    "\t\t\tpreds = numpy.zeros([len(pred_dataset), 2])\n",
    "\t\telse :\n",
    "\t\t\tpreds = numpy.zeros([len(pred_dataset), 3])\n",
    "\n",
    "\t\tattention_scores = numpy.zeros([len(pred_dataset), 12, args.max_seq_length, args.max_seq_length])\n",
    "\n",
    "\t\tfor index, batch in enumerate(tqdm(pred_dataloader, desc = 'Predicting')) :\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tbatch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "\t\t\twith torch.no_grad() :\n",
    "\t\t\t\tinputs = {\n",
    "\t\t\t\t\t'input_ids'      : batch[0],\n",
    "\t\t\t\t\t'attention_mask' : batch[1],\n",
    "\t\t\t\t\t'labels'         : batch[3]\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\t\t\t# XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "\t\t\t\tif args.model_type != 'distilbert' :\n",
    "\t\t\t\t\tinputs['token_type_ids'] = (batch[2] if args.model_type in TOKEN_ID_GROUP else None)\n",
    "\n",
    "\t\t\t\toutputs   = model(**inputs)\n",
    "\t\t\t\tattention = outputs[-1][-1]\n",
    "\t\t\t\t_, logits = outputs[:2]\n",
    "\n",
    "\t\t\t\tpreds[index * batch_size:index * batch_size + len(batch[0]), :] = logits.detach().cpu().numpy()\n",
    "\t\t\t\tattention_scores[index * batch_size:index * batch_size + len(batch[0]), :, :, :] = attention.cpu().numpy()\n",
    "\n",
    "\t\t#\n",
    "\t\t# Alaways applies Softmax no mather the task (so not good for regression)\n",
    "\t\t#\n",
    "\n",
    "\t\tif args.task_name != 'dnasplice' :\n",
    "\t\t\tprobs = softmax(torch.tensor(preds, dtype = torch.float32))[:, 1].numpy()\n",
    "\t\telse :\n",
    "\t\t\tprobs = softmax(torch.tensor(preds, dtype = torch.float32)).numpy()\n",
    "\n",
    "\t\tscores = numpy.zeros([attention_scores.shape[0], attention_scores.shape[-1]])\n",
    "\n",
    "\t\tfor index, attention_score in enumerate(attention_scores) :\n",
    "\t\t\tattn_score = []\n",
    "\n",
    "\t\t\tfor i in range(1, attention_score.shape[-1] - kmer + 2) :\n",
    "\t\t\t\tattn_score.append(float(attention_score[:, 0, i].sum()))\n",
    "\n",
    "\t\t\tfor i in range(len(attn_score) - 1) :\n",
    "\t\t\t\tif attn_score[i + 1] == 0 :\n",
    "\t\t\t\t\tattn_score[i] = 0\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\tcounts      = numpy.zeros([len(attn_score) + kmer - 1])\n",
    "\t\t\treal_scores = numpy.zeros([len(attn_score) + kmer - 1])\n",
    "\n",
    "\t\t\tfor i, score in enumerate(attn_score) :\n",
    "\t\t\t\tfor j in range(kmer) :\n",
    "\t\t\t\t\tcounts[i + j] += 1.0\n",
    "\t\t\t\t\treal_scores[i + j] += score\n",
    "\n",
    "\t\t\treal_scores = real_scores / counts\n",
    "\t\t\treal_scores = real_scores / numpy.linalg.norm(real_scores)\n",
    "\n",
    "\t\t\tscores[index] = real_scores\n",
    "\n",
    "\treturn scores, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2ef5b-473e-49b4-948f-08cf20a448d4",
   "metadata": {},
   "source": [
    "# 9. Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd4ef9-4070-4376-9db2-e36b2db1acbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_cache_examples (args, task, tokenizer, evaluate = False) :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\tif args.local_rank not in [-1, 0] and not evaluate :\n",
    "\t\ttorch.distributed.barrier()\n",
    "\n",
    "\tprocessor   = processors[task]()\n",
    "\toutput_mode = output_modes[task]\n",
    "\n",
    "\t#\n",
    "\t# Load data features from cache or dataset file\n",
    "\t#\n",
    "\n",
    "\tcached_features_file = os.path.join(\n",
    "\t\targs.data_dir,\n",
    "\t\t'cached_{}_{}_{}_{}'.format(\n",
    "\t\t\t'dev' if evaluate else 'train',\n",
    "\t\t\tlist(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
    "\t\t\tstr(args.max_seq_length),\n",
    "\t\t\tstr(task),\n",
    "\t\t),\n",
    "\t)\n",
    "\n",
    "\tif args.do_predict :\n",
    "\t\tcached_features_file = os.path.join(\n",
    "\t\t\targs.data_dir,\n",
    "\t\t\t'cached_{}_{}_{}'.format(\n",
    "\t\t\t\t'dev' if evaluate else 'train',\n",
    "\t\t\t\tstr(args.max_seq_length),\n",
    "\t\t\t\tstr(task),\n",
    "\t\t\t),\n",
    "\t\t)\n",
    "\n",
    "\tif os.path.exists(cached_features_file) and not args.overwrite_cache :\n",
    "\t\tlogger.info('Loading features from cached file %s', cached_features_file)\n",
    "\t\tfeatures = torch.load(cached_features_file)\n",
    "\telse :\n",
    "\t\tlogger.info('Creating features from dataset file at %s', args.data_dir)\n",
    "\t\tlabel_list = processor.get_labels()\n",
    "\n",
    "\t\tif task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta', 'xlmroberta'] :\n",
    "\t\t\tlabel_list[1], label_list[2] = label_list[2], label_list[1]\n",
    "\n",
    "\t\texamples = (\n",
    "\t\t\tprocessor.get_dev_examples(args.data_dir)\n",
    "\t\t\tif evaluate\n",
    "\t\t\telse processor.get_train_examples(args.data_dir)\n",
    "\t\t)\n",
    "\n",
    "\t\t# Params for convert_examples_to_features\n",
    "\t\tmax_length  = args.max_seq_length\n",
    "\t\tpad_on_left = bool(args.model_type in ['xlnet'])\n",
    "\t\tpad_token   = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "\n",
    "\t\tif args.model_type in ['xlnet'] :\n",
    "\t\t\tpad_token_segment_id = 4\n",
    "\t\telse :\n",
    "\t\t\tpad_token_segment_id = 0\n",
    "\n",
    "\t\tif args.n_process == 1 :\n",
    "\t\t\tfeatures = convert_examples_to_features(\n",
    "\t\t\t\texamples,\n",
    "\t\t\t\ttokenizer,\n",
    "\t\t\t\tlabel_list  = label_list,\n",
    "\t\t\t\tmax_length  = max_length,\n",
    "\t\t\t\toutput_mode = output_mode,\n",
    "\t\t\t\tpad_on_left = pad_on_left,\n",
    "\t\t\t\tpad_token   = pad_token,\n",
    "\t\t\t\tpad_token_segment_id = pad_token_segment_id,\n",
    "\t\t\t)\n",
    "\n",
    "\t\telse :\n",
    "\t\t\tnproc = int(args.n_process)\n",
    "\n",
    "\t\t\tif evaluate :\n",
    "\t\t\t\tnproc = max(int(nproc / 4), 1)\n",
    "\n",
    "\t\t\tprint('Number of processes for converting feature: ' + str(nproc))\n",
    "\n",
    "\t\t\tpool      = Pool(nproc)\n",
    "\t\t\tindexes   = [0]\n",
    "\t\t\tlen_slice = int(len(examples) / nproc)\n",
    "\n",
    "\t\t\tfor i in range(1, nproc + 1) :\n",
    "\t\t\t\tif i != nproc:\n",
    "\t\t\t\t\tindexes.append(len_slice * (i))\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tindexes.append(len(examples))\n",
    "\n",
    "\t\t\tresults  = list()\n",
    "\t\t\tfeatures = list()\n",
    "\n",
    "\t\t\tfor i in range(nproc) :\n",
    "\t\t\t\tresults.append(pool.apply_async(\n",
    "\t\t\t\t\tconvert_examples_to_features,\n",
    "\t\t\t\t\targs = (\n",
    "\t\t\t\t\t\texamples[indexes[i]:indexes[i + 1]],\n",
    "\t\t\t\t\t\ttokenizer, max_length, None, label_list, output_mode,\n",
    "\t\t\t\t\t\tpad_on_left, pad_token, pad_token_segment_id, True\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t))\n",
    "\t\t\t\tprint(str(i + 1) + ' processor started !')\n",
    "\n",
    "\t\t\tpool.close()\n",
    "\t\t\tpool.join()\n",
    "\n",
    "\t\t\tfor result in results :\n",
    "\t\t\t\tfeatures.extend(result.get())\n",
    "\n",
    "\t\tif args.local_rank in [-1, 0] :\n",
    "\t\t\tlogger.info('Saving features into cached file %s', cached_features_file)\n",
    "\t\t\ttorch.save(features, cached_features_file)\n",
    "\n",
    "\t# Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\tif args.local_rank == 0 and not evaluate :\n",
    "\t\ttorch.distributed.barrier()\n",
    "\n",
    "\t# Convert to tensors and build dataset\n",
    "\tall_input_ids      = torch.tensor([f.input_ids      for f in features], dtype = torch.long)\n",
    "\tall_attention_mask = torch.tensor([f.attention_mask for f in features], dtype = torch.long)\n",
    "\tall_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype = torch.long)\n",
    "\n",
    "\tif output_mode == 'classification' :\n",
    "\t\tall_labels = torch.tensor([f.label for f in features], dtype = torch.long)\n",
    "\telif output_mode == 'regression' :\n",
    "\t\tall_labels = torch.tensor([f.label for f in features], dtype = torch.float)\n",
    "\n",
    "\treturn TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc83f8-24b3-441a-be15-4d0076f4ebd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 10. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0f127-8646-4e6e-96e3-391c966c2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main method\n",
    "\n",
    "def main () :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\tparser  =  argparse.ArgumentParser()\n",
    "\n",
    "\t# Required parameters\n",
    "\tparser.add_argument('--data_dir',           default = None, type = str, required = True, help = 'The input data dir. Should contain the .tsv files (or other data files) for the task.')\n",
    "\tparser.add_argument('--model_type',         default = None, type = str, required = True, help = 'Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n",
    "\tparser.add_argument('--model_name_or_path', default = None, type = str, required = True, help = 'Path to pre-trained model or shortcut name selected in the list: ' + ', '.join(ALL_MODELS))\n",
    "\tparser.add_argument('--task_name',          default = None, type = str, required = True, help = 'The name of the task to train selected in the list: ' + ', '.join(processors.keys()))\n",
    "\tparser.add_argument('--output_dir',         default = None, type = str, required = True, help = 'The output directory where the model predictions and checkpoints will be written.')\n",
    "\n",
    "\t# Other parameters\n",
    "\tparser.add_argument('--n_process',                    default = 2,      type = int,   help = 'The number of processes used for data process')\n",
    "\tparser.add_argument('--visualize_data_dir',           default = None,   type = str,   help = 'The input data dir. Should contain the .tsv files for the task.')\n",
    "\tparser.add_argument('--result_dir',                   default = None,   type = str,   help = 'The directory where the dna690 and mouse will save results.')\n",
    "\tparser.add_argument('--config_name',                  default = '',     type = str,   help = 'Pretrained config name or path if not the same as model_name')\n",
    "\tparser.add_argument('--tokenizer_name',               default = '',     type = str,   help = 'Pretrained tokenizer name or path if not the same as model_name')\n",
    "\tparser.add_argument('--cache_dir',                    default = '',     type = str,   help = 'Where do you want to store the pre-trained models downloaded from s3')\n",
    "\tparser.add_argument('--predict_dir',                  default = None,   type = str,   help = 'The output directory of predicted result. (when do_predict)')\n",
    "\tparser.add_argument('--max_seq_length',               default = 128,    type = int,   help = 'The maximum total input sequence length after tokenization.')\n",
    "\tparser.add_argument('--per_gpu_train_batch_size',     default = 8,      type = int,   help = 'Batch size per GPU/CPU for training.')\n",
    "\tparser.add_argument('--per_gpu_eval_batch_size',      default = 8,      type = int,   help = 'Batch size per GPU/CPU for evaluation.')\n",
    "\tparser.add_argument('--per_gpu_pred_batch_size',      default = 8,      type = int,   help = 'Batch size per GPU/CPU for prediction.')\n",
    "\tparser.add_argument('--early_stop',                   default = 0,      type = int,   help = 'set this to a positive integet if you want to perfrom early stop.')\n",
    "\tparser.add_argument('--predict_scan_size',            default = 1,      type = int,   help = 'Number of updates steps to accumulate before performing a backward/update pass.')\n",
    "\tparser.add_argument('--gradient_accumulation_steps',  default = 1,      type = int,   help = 'Number of updates steps to accumulate before performing a backward/update pass.')\n",
    "\tparser.add_argument('--learning_rate',                default = 5e-5,   type = float, help = 'The initial learning rate for Adam.')\n",
    "\tparser.add_argument('--weight_decay',                 default = 0.0,    type = float, help = 'Weight decay if we apply some.')\n",
    "\tparser.add_argument('--adam_epsilon',                 default = 1e-8,   type = float, help = 'Epsilon for Adam optimizer.')\n",
    "\tparser.add_argument('--beta1',                        default = 0.9,    type = float, help = 'Beta1 for Adam optimizer.')\n",
    "\tparser.add_argument('--beta2',                        default = 0.999,  type = float, help = 'Beta2 for Adam optimizer.')\n",
    "\tparser.add_argument('--max_grad_norm',                default = 1.0,    type = float, help = 'Max gradient norm.')\n",
    "\tparser.add_argument('--attention_probs_dropout_prob', default = 0.1,    type = float, help = 'Dropout rate of attention.')\n",
    "\tparser.add_argument('--hidden_dropout_prob',          default = 0.1,    type = float, help = 'Dropout rate of intermidiete layer.')\n",
    "\tparser.add_argument('--rnn_dropout',                  default = 0.0,    type = float, help = 'Dropout rate of intermidiete layer.')\n",
    "\tparser.add_argument('--rnn',                          default = 'lstm', type = str,   help = 'What kind of RNN to use')\n",
    "\tparser.add_argument('--num_rnn_layer',                default = 2,      type = int,   help = 'Number of rnn layers in dnalong model.')\n",
    "\tparser.add_argument('--rnn_hidden',                   default = 768,    type = int,   help = 'Number of hidden unit in a rnn layer.')\n",
    "\tparser.add_argument('--num_train_epochs',             default = 3.0,    type = float, help = 'Total number of training epochs to perform.')\n",
    "\tparser.add_argument('--max_steps',                    default = -1,     type = int,   help = 'If > 0: set total number of training steps to perform. Override num_train_epochs.')\n",
    "\tparser.add_argument('--warmup_steps',                 default = 0,      type = int,   help = 'Linear warmup over warmup_steps.')\n",
    "\tparser.add_argument('--warmup_percent',               default = 0,      type = float, help = 'Linear warmup over warmup_percent*total_steps.')\n",
    "\tparser.add_argument('--logging_steps',                default = 500,    type = int,   help = 'Log every X updates steps.')\n",
    "\tparser.add_argument('--save_steps',                   default = 500,    type = int,   help = 'Save checkpoint every X updates steps.')\n",
    "\tparser.add_argument('--save_total_limit',             default = None,   type = int,   help = 'Limit the total amount of checkpoints.')\n",
    "\tparser.add_argument('--visualize_models',             default = None,   type = int,   help = 'The model used to do visualization. If None, use 3456.')\n",
    "\tparser.add_argument('--seed',                         default = 42,     type = int,   help = 'random seed for initialization')\n",
    "\tparser.add_argument('--fp16_opt_level',               default = 'O1',   type = str,   help = 'For fp16 see details at https://nvidia.github.io/apex/amp.html')\n",
    "\tparser.add_argument('--local_rank',                   default = -1,     type = int,   help = 'For distributed training: local_rank')\n",
    "\tparser.add_argument('--server_ip',                    default = '',     type = str,   help = 'For distant debugging.')\n",
    "\tparser.add_argument('--server_port',                  default = '',     type = str,   help = 'For distant debugging.')\n",
    "\n",
    "\tparser.add_argument('--should_continue',          action = 'store_true', help = 'Whether to continue from latest checkpoint in output_dir')\n",
    "\tparser.add_argument('--do_train',                 action = 'store_true', help = 'Whether to run training.')\n",
    "\tparser.add_argument('--do_eval',                  action = 'store_true', help = 'Whether to run eval on the dev set.')\n",
    "\tparser.add_argument('--do_predict',               action = 'store_true', help = 'Whether to do prediction on the given dataset.')\n",
    "\tparser.add_argument('--do_visualize',             action = 'store_true', help = 'Whether to calculate attention score.')\n",
    "\tparser.add_argument('--visualize_train',          action = 'store_true', help = 'Whether to visualize train.tsv or dev.tsv.')\n",
    "\tparser.add_argument('--do_ensemble_pred',         action = 'store_true', help = 'Whether to do ensemble prediction with kmer 3456.')\n",
    "\tparser.add_argument('--evaluate_during_training', action = 'store_true', help = 'Run evaluation during training at each logging step.')\n",
    "\tparser.add_argument('--do_lower_case',            action = 'store_true', help = 'Set this flag if you are using an uncased model.')\n",
    "\tparser.add_argument('--eval_all_checkpoints',     action = 'store_true', help = 'Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n",
    "\tparser.add_argument('--no_cuda',                  action = 'store_true', help = 'Avoid using CUDA when available')\n",
    "\tparser.add_argument('--overwrite_output_dir',     action = 'store_true', help = 'Overwrite the content of the output directory')\n",
    "\tparser.add_argument('--overwrite_cache',          action = 'store_true', help = 'Overwrite the cached training and evaluation sets')\n",
    "\tparser.add_argument('--fp16',                     action = 'store_true', help = 'Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\tif args.should_continue :\n",
    "\t\tsorted_checkpoints = _sorted_checkpoints(args)\n",
    "\n",
    "\t\tif len(sorted_checkpoints) == 0 :\n",
    "\t\t\traise ValueError('Used --should_continue but no checkpoint was found in --output_dir.')\n",
    "\t\telse :\n",
    "\t\t\targs.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "\tif (os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir) :\n",
    "\t\traise ValueError('Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.'.format(args.output_dir))\n",
    "\n",
    "\tif args.server_ip and args.server_port:\n",
    "\t\timport ptvsd\n",
    "\n",
    "\t\tptvsd.enable_attach(address = (args.server_ip, args.server_port), redirect_output = True)\n",
    "\t\tptvsd.wait_for_attach()\n",
    "\n",
    "\tif args.local_rank == -1 or args.no_cuda :\n",
    "\t\tdevice = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
    "\t\targs.n_gpu = torch.cuda.device_count()\n",
    "\telse :\n",
    "\t\ttorch.cuda.set_device(args.local_rank)\n",
    "\t\tdevice = torch.device('cuda', args.local_rank)\n",
    "\t\ttorch.distributed.init_process_group(backend = 'nccl')\n",
    "\t\targs.n_gpu = 1\n",
    "\n",
    "\targs.device = device\n",
    "\n",
    "\tlogging.basicConfig(\n",
    "\t\tformat  = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "\t\tdatefmt = '%m/%d/%Y %H:%M:%S',\n",
    "\t\tlevel  = logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "\t)\n",
    "\n",
    "\tlogger.warning(\n",
    "\t\t'Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s',\n",
    "\t\targs.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16\n",
    "\t)\n",
    "\n",
    "\tset_seed(args)\n",
    "\n",
    "\t#\n",
    "\t# Prepare GLUE task\n",
    "\t#\n",
    "\n",
    "\targs.task_name = args.task_name.lower()\n",
    "\n",
    "\tif args.task_name not in processors :\n",
    "\t\traise ValueError('Task not found: %s' % (args.task_name))\n",
    "\n",
    "\tprocessor        = processors[args.task_name]()\n",
    "\targs.output_mode = output_modes[args.task_name]\n",
    "\tlabel_list       = processor.get_labels()\n",
    "\tnum_labels       = len(label_list)\n",
    "\n",
    "\t# Load pretrained model and tokenizer\n",
    "\t# Make sure only the first process in distributed training will download model & vocab\n",
    "\tif args.local_rank not in [-1, 0] :\n",
    "\t\ttorch.distributed.barrier() \n",
    "\n",
    "\targs.model_type = args.model_type.lower()\n",
    "\n",
    "\tconfig_class    = MODEL_CLASSES[args.model_type][0]\n",
    "\tmodel_class     = MODEL_CLASSES[args.model_type][1]\n",
    "\ttokenizer_class = MODEL_CLASSES[args.model_type][2]\n",
    "\n",
    "\tif not args.do_visualize and not args.do_ensemble_pred :\n",
    "\t\tconfig = config_class.from_pretrained(\n",
    "\t\t\targs.config_name if args.config_name else args.model_name_or_path,\n",
    "\t\t\tnum_labels      = num_labels,\n",
    "\t\t\tfinetuning_task = args.task_name,\n",
    "\t\t\tcache_dir       = args.cache_dir if args.cache_dir else None\n",
    "\t\t)\n",
    "\n",
    "\t\tconfig.hidden_dropout_prob          = args.hidden_dropout_prob\n",
    "\t\tconfig.attention_probs_dropout_prob = args.attention_probs_dropout_prob\n",
    "\n",
    "\t\tif args.model_type in ['dnalong', 'dnalongcat'] :\n",
    "\t\t\tassert args.max_seq_length % 512 == 0\n",
    "\n",
    "\t\tconfig.split         = int(args.max_seq_length / 512)\n",
    "\t\tconfig.rnn           = args.rnn\n",
    "\t\tconfig.num_rnn_layer = args.num_rnn_layer\n",
    "\t\tconfig.rnn_dropout   = args.rnn_dropout\n",
    "\t\tconfig.rnn_hidden    = args.rnn_hidden\n",
    "\n",
    "\t\ttokenizer = tokenizer_class.from_pretrained(\n",
    "\t\t\targs.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "\t\t\tdo_lower_case = args.do_lower_case,\n",
    "\t\t\tcache_dir     = args.cache_dir if args.cache_dir else None,\n",
    "\t\t)\n",
    "\n",
    "\t\tmodel = model_class.from_pretrained(\n",
    "\t\t\targs.model_name_or_path,\n",
    "\t\t\tfrom_tf   = bool(\".ckpt\" in args.model_name_or_path),\n",
    "\t\t\tconfig    = config,\n",
    "\t\t\tcache_dir = args.cache_dir if args.cache_dir else None,\n",
    "\t\t)\n",
    "\n",
    "\t\tlogger.info('Finish loading model')\n",
    "\n",
    "\t\t# Make sure only the first process in distributed training will download model & vocab\n",
    "\t\tif args.local_rank == 0 :\n",
    "\t\t\ttorch.distributed.barrier()\n",
    "\n",
    "\t\tmodel.to(args.device)\n",
    "\n",
    "\t\tlogger.info('Training/evaluation parameters %s', args)\n",
    "\n",
    "\t#\n",
    "\t# Training\n",
    "\t#\n",
    "\n",
    "\tif args.do_train :\n",
    "\t\ttrain_dataset        = load_and_cache_examples(args, args.task_name, tokenizer, evaluate = False)\n",
    "\t\tglobal_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "\n",
    "\t\tlogger.info('global_step = %s, average loss = %s', global_step, tr_loss)\n",
    "\n",
    "\n",
    "\tif args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0) and args.task_name != 'dna690' :\n",
    "\t\tif not os.path.exists(args.output_dir) and args.local_rank in [-1, 0] :\n",
    "\t\t\tos.makedirs(args.output_dir)\n",
    "\n",
    "\t\tlogger.info('Saving model checkpoint to %s', args.output_dir)\n",
    "\n",
    "\t\t# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "\t\t# They can then be reloaded using `from_pretrained()`\n",
    "\t\t# Good practice: save your training arguments together with the trained model\n",
    "\t\tmodel_to_save = (model.module if hasattr(model, 'module') else model)\n",
    "\t\tmodel_to_save.save_pretrained(args.output_dir)\n",
    "\t\ttokenizer.save_pretrained(args.output_dir)\n",
    "\t\ttorch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
    "\n",
    "\t\t# Load a trained model and vocabulary that you have fine-tuned\n",
    "\t\tmodel     = model_class.from_pretrained(args.output_dir)\n",
    "\t\ttokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
    "\n",
    "\t\tmodel.to(args.device)\n",
    "\n",
    "\t#\n",
    "\t# Evaluation\n",
    "\t#\n",
    "\n",
    "\tresults = dict()\n",
    "\n",
    "\tif args.do_eval and args.local_rank in [-1, 0] :\n",
    "\t\ttokenizer   = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "\t\tcheckpoints = [args.output_dir]\n",
    "\n",
    "\t\tif args.eval_all_checkpoints :\n",
    "\t\t\tcheckpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive = True)))\n",
    "\n",
    "\t\t\t# Reduce logging\n",
    "\t\t\tlogging.getLogger('transformers.modeling_utils').setLevel(logging.WARN) \n",
    "\n",
    "\t\tlogger.info('Evaluate the following checkpoints: %s', checkpoints)\n",
    "\n",
    "\t\tfor checkpoint in checkpoints :\n",
    "\t\t\tglobal_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n",
    "\t\t\tprefix      = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n",
    "\n",
    "\t\t\tmodel = model_class.from_pretrained(checkpoint)\n",
    "\t\t\tmodel.to(args.device)\n",
    "\n",
    "\t\t\tresult = evaluate(args, model, tokenizer, prefix = prefix)\n",
    "\t\t\tresult = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "\n",
    "\t\t\tresults.update(result)\n",
    "\n",
    "\t#\n",
    "\t# Prediction\n",
    "\t#\n",
    "\n",
    "\tpredictions = dict()\n",
    "\n",
    "\tif args.do_predict and args.local_rank in [-1, 0] :\n",
    "\t\ttokenizer  = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "\t\tcheckpoint = args.output_dir\n",
    "\t\tprefix     = ''\n",
    "\n",
    "\t\tlogger.info('Predict using the following checkpoint: %s', checkpoint)\n",
    "\n",
    "\t\tmodel = model_class.from_pretrained(checkpoint)\n",
    "\t\tmodel.to(args.device)\n",
    "\n",
    "\t\tprediction = predict(args, model, tokenizer, prefix = prefix)\n",
    "\n",
    "\t#\n",
    "\t# Visualize\n",
    "\t#\n",
    "\n",
    "\tif args.do_visualize and args.local_rank in [-1, 0] :\n",
    "\t\tvisualization_models = [3, 4, 5, 6] if not args.visualize_models else [args.visualize_models]\n",
    "\n",
    "\t\tscores    = None\n",
    "\t\tall_probs = None\n",
    "\n",
    "\t\tfor kmer in visualization_models :\n",
    "\t\t\toutput_dir = args.output_dir.replace('/690', '/690/' + str(kmer))\n",
    "\n",
    "\t\t\ttokenizer = tokenizer_class.from_pretrained(\n",
    "\t\t\t\t'dna' + str(kmer),\n",
    "\t\t\t\tdo_lower_case = args.do_lower_case,\n",
    "\t\t\t\tcache_dir     = args.cache_dir if args.cache_dir else None,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tcheckpoint = output_dir\n",
    "\t\t\tlogger.info('Calculate attention score using the following checkpoint: %s', checkpoint)\n",
    "\t\t\tprefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n",
    "\n",
    "\t\t\tconfig = config_class.from_pretrained(\n",
    "\t\t\t\toutput_dir,\n",
    "\t\t\t\tnum_labels      = num_labels,\n",
    "\t\t\t\tfinetuning_task = args.task_name,\n",
    "\t\t\t\tcache_dir       = args.cache_dir if args.cache_dir else None,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tconfig.output_attentions = True\n",
    "\n",
    "\t\t\tmodel = model_class.from_pretrained(\n",
    "\t\t\t\tcheckpoint,\n",
    "\t\t\t\tfrom_tf   = bool('.ckpt' in args.model_name_or_path),\n",
    "\t\t\t\tconfig    = config,\n",
    "\t\t\t\tcache_dir = args.cache_dir if args.cache_dir else None,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tmodel.to(args.device)\n",
    "\t\t\tattention_scores, probs = visualize(args, model, tokenizer, prefix = prefix, kmer = kmer)\n",
    "\n",
    "\t\t\tif scores is not None :\n",
    "\t\t\t\tall_probs += probs\n",
    "\t\t\t\tscores    += attention_scores\n",
    "\t\t\telse :\n",
    "\t\t\t\tall_probs = deepcopy(probs)\n",
    "\t\t\t\tscores    = deepcopy(attention_scores)\n",
    "\n",
    "\t\tall_probs = all_probs / float(len(visualization_models))\n",
    "\n",
    "\t\tnumpy.save(os.path.join(args.predict_dir, 'atten.npy'), scores)\n",
    "\t\tnumpy.save(os.path.join(args.predict_dir, 'pred_results.npy'), all_probs)\n",
    "\n",
    "\t#\n",
    "\t# Ensemble\n",
    "\t#\n",
    "\n",
    "\tif args.do_ensemble_pred and args.local_rank in [-1, 0] :\n",
    "\t\tfor kmer in range(3, 7) :\n",
    "\t\t\toutput_dir = os.path.join(args.output_dir, str(kmer))\n",
    "\t\t\ttokenizer  = tokenizer_class.from_pretrained(\n",
    "\t\t\t\t'dna' + str(kmer),\n",
    "\t\t\t\tdo_lower_case = args.do_lower_case,\n",
    "\t\t\t\tcache_dir     = args.cache_dir if args.cache_dir else None\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tcheckpoint = output_dir\n",
    "\t\t\tlogger.info('Calculate attention score using the following checkpoint: %s', checkpoint)\n",
    "\t\t\tprefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n",
    "\n",
    "\t\t\tconfig = config_class.from_pretrained(\n",
    "\t\t\t\toutput_dir,\n",
    "\t\t\t\tnum_labels      = num_labels,\n",
    "\t\t\t\tfinetuning_task = args.task_name,\n",
    "\t\t\t\tcache_dir       = args.cache_dir if args.cache_dir else None\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tconfig.output_attentions = True\n",
    "\n",
    "\t\t\tmodel = model_class.from_pretrained(\n",
    "\t\t\t\targs.model_name_or_path,\n",
    "\t\t\t\tfrom_tf   = bool('.ckpt' in args.model_name_or_path),\n",
    "\t\t\t\tconfig    = config,\n",
    "\t\t\t\tcache_dir = args.cache_dir if args.cache_dir else None,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tmodel.to(args.device)\n",
    "\n",
    "\t\t\tif kmer == 3 :\n",
    "\t\t\t\targs.data_dir = os.path.join(args.data_dir, str(kmer))\n",
    "\t\t\telse:\n",
    "\t\t\t\targs.data_dir = args.data_dir.replace('/' + str(kmer - 1), '/' + str(kmer))\n",
    "\n",
    "\t\t\tif args.result_dir.split('/')[-1] == 'test.npy' :\n",
    "\t\t\t\tresults, eval_task, _, out_label_ids, probs = evaluate(args, model, tokenizer, prefix = prefix)\n",
    "\t\t\telif args.result_dir.split('/')[-1] == 'train.npy':\n",
    "\t\t\t\tresults, eval_task, _, out_label_ids, probs = evaluate(args, model, tokenizer, prefix = prefix, evaluate = False)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError('File name in result_dir should be either test.npy or train.npy')\n",
    "\n",
    "\t\t\tif kmer == 3 :\n",
    "\t\t\t\tall_probs = deepcopy(probs)\n",
    "\t\t\t\tcat_probs = deepcopy(probs)\n",
    "\t\t\telse :\n",
    "\t\t\t\tall_probs += probs\n",
    "\t\t\t\tcat_probs = numpy.concatenate((cat_probs, probs), axis = 1)\n",
    "\n",
    "\t\t\tprint(cat_probs[0])\n",
    "\n",
    "\t\tall_probs = all_probs / 4.0\n",
    "\t\tall_preds = numpy.argmax(all_probs, axis = 1)\n",
    "\n",
    "\t\t# Save label and data for stuck ensemble\n",
    "\t\tlabels = numpy.array(out_label_ids)\n",
    "\t\tlabels = labels.reshape(labels.shape[0], 1)\n",
    "\t\tdata   = numpy.concatenate((cat_probs, labels), axis = 1)\n",
    "\n",
    "\t\trandom.shuffle(data)\n",
    "\n",
    "\t\troot_path = args.result_dir.replace(args.result_dir.split('/')[-1], '')\n",
    "\n",
    "\t\tif not os.path.exists(root_path) :\n",
    "\t\t\tos.makedirs(root_path)\n",
    "\n",
    "\t\tnumpy.save(args.result_dir, data)\n",
    "\n",
    "\t\tensemble_results = compute_metrics(eval_task, all_preds, out_label_ids, all_probs[:, 1])\n",
    "\n",
    "\t\tlogger.info('***** Ensemble results {} *****'.format(prefix))\n",
    "\n",
    "\t\tfor key in sorted(ensemble_results.keys()) :\n",
    "\t\t\tlogger.info('  %s = %s', key, str(ensemble_results[key]))\n",
    "\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16001e-b1d9-4a75-85e4-e554e430d25b",
   "metadata": {},
   "source": [
    "# 11. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec89ea-2b49-412b-bca8-632a7540a520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing RegressionProcessor vs DnaPromProcessor vs StsbProcessor\n",
    "\n",
    "def test_processor () :\n",
    "\t\"\"\"\n",
    "\tDoc\n",
    "\t\"\"\"\n",
    "\n",
    "\tprocessor0 = DnaPromProcessor()\n",
    "\tprocessor1 = RegressionProcessor()\n",
    "\n",
    "\tpath = os.path.join(ROOT, 'source', 'python', 'bert', 'dnabert', 'examples', 'sample_data', 'ft', '6')\n",
    "\n",
    "\texamples0 = processor0.get_train_examples(path)\n",
    "\texamples1 = processor1.get_train_examples(path)\n",
    "\n",
    "\tfor name, examples in zip(['DnaProm', 'Regression'], [examples0, examples1]) :\n",
    "\t\tprint()\n",
    "\t\tprint(name + 'Processor')\n",
    "\t\tprint('GUID  : {}'.format(examples[0].guid))\n",
    "\t\tprint('Text  : {} ... {}'.format(examples[0].text_a[:30], examples[0].text_a[-30:]))\n",
    "\t\tprint('Label : {}'.format(examples[0].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3df0f1-69e1-4419-83e4-cfcf681b756c",
   "metadata": {},
   "source": [
    "# 12. Launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482b9ad-013f-41df-ac48-63d2ff5a32ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and 'get_ipython' in dir() :\n",
    "\tprint('Running as .ipynb')\n",
    "\n",
    "\ttest_processor()\n",
    "\n",
    "if __name__ == '__main__' and 'get_ipython' not in dir() :\n",
    "\tprint('Running as .py')\n",
    "\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
